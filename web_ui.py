# Copyright (c) Alibaba Cloud.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
import json
import os
from argparse import ArgumentParser

import copy

import gradio as gr
import os
import re
import sys
import logging
import base64
import requests
import random
import string
from altair import value
from httpx import Timeout
import itertools

from llms import QwenVLChatModel, GLM4VModel, InternVLModel
from utils import minio_util
from config import VLLM_ENDPOINT, LM_DEPLOY_ENDPOINT

API_KEY = "EMPTY"

DEFAULT_TIMEOUT_CONFIG = Timeout(timeout=30.0)
DEFAULT_CKPT_PATH = 'qwen/Qwen-VL-Chat'
REVISION = 'v1.0.4'
BOX_TAG_PATTERN = r"<box>([\s\S]*?)</box>"
PUNCTUATION = "ï¼ï¼Ÿã€‚ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï½Ÿï½ ï½¢ï½£ï½¤ã€ã€ƒã€‹ã€Œã€ã€ã€ã€ã€‘ã€”ã€•ã€–ã€—ã€˜ã€™ã€šã€›ã€œã€ã€ã€Ÿã€°ã€¾ã€¿â€“â€”â€˜â€™â€›â€œâ€â€â€Ÿâ€¦â€§ï¹."
# DEFAULT_PROMPT_TEMPLATE = """
# ä½ æ˜¯ä¸€åå…¬è·¯å·¡æ£€å…»æŠ¤æ ¡éªŒä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·æä¾›çš„ç—…å®³æè¿°ï¼Œåˆ¤æ–­ç—…å®³æè¿°æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è¿”å›ç»“æœã€‚
# ä»»åŠ¡è¦æ±‚ï¼š
# 1. å¦‚æœç—…å®³æè¿°æ­£ç¡®ï¼Œè¿”å›"check"=trueï¼Œç¤ºä¾‹ï¼š{"check" true, â€œtypeâ€: "ç—…å®³ç±»å‹""}
# 2. å¦‚æœç—…å®³æè¿°ä¸æ­£ç¡®ï¼Œè¿”å›"check"=false, ç¤ºä¾‹ï¼š{â€œcheckâ€: false, â€œtypeâ€: â€œç—…å®³ç±»å‹â€}
# 3. ç—…å®³ç±»å‹typeåŒ…æ‹¬ï¼š["æ— ç—…å®³", "çºµå‘è£‚éš™", "æ¨ªå‘è£‚éš™", "ç½‘çŠ¶è£‚éš™", "å—çŠ¶è£‚ç¼", "äº•ç›–ç ´æŸ", "äº•ç›–ç¼ºå¤±", "æŠ›æ´’ç‰©", "ç§¯æ°´", "å‘æ§½", "é¾Ÿè£‚"]
# 4. å¦‚æ— æ˜æ˜¾ç—…å®³ï¼Œåˆ™type="æ— ç—…å®³"ï¼Œç¤ºä¾‹ï¼š{"check": "æ£€æµ‹ç»“æœ", â€œtypeâ€: "æ— ç—…å®³"}
# 5. è¿”å›ç»“æœå¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼
# 6. ä»…è¿”å›JSONç»“æœï¼Œä¸éœ€è¦é¢å¤–å†…å®¹
# 7. typeä¸ºæ£€æµ‹ç—…å®³ç±»å‹ï¼Œå¦‚æœæ£€æµ‹ç—…å®³ç±»å‹ä¸ç”¨æˆ·ç—…å®³ç±»å‹ä¸ä¸€è‡´ï¼Œè¯·è¾“å‡ºæ£€æµ‹ç—…å®³ç±»å‹
#
# {user_input}
#
# """

DEFAULT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åå…¬è·¯å·¡æ£€å…»æŠ¤æ ¡éªŒä¸“å®¶ï¼Œæ£€æŸ¥å›¾ç‰‡ä¸­çš„é»„è‰²æ¡†æ˜¯å¦å­˜åœ¨ç—…å®³ï¼Œä»…å…³æœºåŠ¨è½¦æ³¨é“è·¯ç—…å®³ï¼Œè·¯é¢åŸºç¡€å»ºç­‘ä»¥åŠäººç‰©ã€æ¤è¢«è¯·å¿½ç•¥ã€‚

è§„åˆ™ï¼š
1. ç§æ¤çš„æ ‘æœ¨ä¸‹æ–¹çš„å‘éå‘æ§½ï¼Œä¸ºæ— ç—…å®³ã€‚
2. å‘æ§½ï¼šå¯èƒ½å‘ˆç°å‡ºåœ†å½¢ã€æ¤­åœ†å½¢æˆ–ä¸è§„åˆ™å½¢çŠ¶ï¼Œå‘¨å›´çš„è¾¹ç¼˜å¯èƒ½ä¼šå‘ˆç°ç ´ç¢çŠ¶æˆ–è£‚çº¹çŠ¶æ€ï¼Œä¸è¦å¿½ç•¥å°å‘çš„å­˜åœ¨ã€‚
3. é“è·¯ä¸Šçš„åƒåœ¾ã€å¡‘æ–™è¢‹ã€çƒŸç›’ã€ç¢çº¸ã€æ¯›çº¿ç­‰å¼‚å¸¸ç‰©ä½“å‡ä¸ºæŠ›æ´’ç‰©ç±»å‹ã€‚
4. å‘æ§½å’ŒæŠ›æ´’ç‰©è¦è°¨æ…åŒºåˆ†ï¼Œå‘æ§½ä¸ºå‡¹é™·çŠ¶æ€ï¼ŒæŠ›æ´’ç‰©ä¸ºå‡¸èµ·çŠ¶æ€ã€‚
5. åå‚ç›´è£…è£‚éš™ä¸ºçºµå‘è£‚éš™ï¼Œåæ°´å¹³è£…è£‚éš™ä¸ºæ¨ªå‘è£‚éš™ã€‚

è¦æ±‚ï¼š
1. å¦‚æœå­˜åœ¨ç—…å®³ï¼Œè¿”å›"check"=trueï¼Œç¤ºä¾‹ï¼š{"check" true, "type": "ç—…å®³ç±»å‹"}
2. å¦‚æœä¸å­˜åœ¨ç—…å®³ï¼Œè¿”å›"check"=false, ç¤ºä¾‹ï¼š{"check": false, "type": "æ— ç—…å®³"}
3. ç—…å®³ç±»å‹typeåŒ…æ‹¬ï¼š["çºµå‘è£‚éš™", "æ¨ªå‘è£‚éš™", "ç½‘çŠ¶è£‚éš™", "å—çŠ¶è£‚ç¼", "äº•ç›–ç ´æŸ", "æŠ›æ´’ç‰©", "ç§¯æ°´", "å‘æ§½"]
4. å¦‚æ— ç—…å®³ï¼Œåˆ™type="æ— ç—…å®³"ï¼Œç¤ºä¾‹ï¼š{"check": false, "type": "æ— ç—…å®³"}

ç»“æœä»¥JSONæ ¼å¼è¿”å›ï¼Œä¸éœ€è¦å…¶ä»–ä»»ä½•å†…å®¹ï¼
{user_input}
"""


class Logger:
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "w")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def isatty(self):
        return False


sys.stdout = Logger("output.log")


def generate_random_string(length=4):
    # å®šä¹‰å¯é€‰å­—ç¬¦é›†ï¼ŒåŒ…æ‹¬å¤§å†™å­—æ¯ã€å°å†™å­—æ¯å’Œæ•°å­—
    characters = string.ascii_letters + string.digits
    # ä½¿ç”¨random.choicesä»charactersä¸­éšæœºé€‰å–lengthä¸ªå­—ç¬¦
    random_string = ''.join(random.choices(characters, k=length))
    return random_string


def test(x):
    print("This is a test")
    print(f"Your function is running with input {x}...")
    return x


def read_logs():
    sys.stdout.flush()
    with open("output.log", "r") as f:
        return f.read()


def _get_args():
    parser = ArgumentParser()
    # parser.add_argument("-c", "--checkpoint-path", type=str, #default=DEFAULT_CKPT_PATH,
    #                    help="Checkpoint name or path, #default to %(default)r")
    # parser.add_argument("--revision", type=str, #default=REVISION)
    # parser.add_argument("--cpu-only", action="store_true", #help="Run demo with CPU only")
    parser.add_argument("--prod", action="store_true", default=False,
                        help="tart prod.")
    parser.add_argument("--share", action="store_true", default=False,
                        help="Create a publicly shareable link for the interface.")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--server-port", type=int, default=9081,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="0.0.0.0",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def split_into_groups(n, m):
    """
    æ ¹æ®è¾“å…¥nï¼Œæœ€å¤§ä¸ªæ•°mï¼Œåˆ†ç»„n
    n=3 m=2  [[0,1],[2]]
    n=5 m=2  [[0,1], [2,3], [4]]
    :param n: è¾“å…¥æ•°å­—
    :param m: æ¯ç»„æœ€å¤§ä¸ªæ•°
    :return:
    """
    # è®¡ç®—æ‰€éœ€çš„æœ€å°‘ç»„æ•°
    k = -(-n // m)  # ä½¿ç”¨è´Ÿæ•°é™¤æ³•å‘ä¸Šå–æ•´
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    groups = []
    # å½“å‰èµ·å§‹ç´¢å¼•
    start_index = 0
    for i in range(k):
        # æ¯ç»„å¤§å°ä¸º m æˆ–è€…æ˜¯å‰©ä½™çš„å…ƒç´ æ•°é‡
        group_size = min(m, n - start_index)
        # åˆ›å»ºå¹¶æ·»åŠ è¿™ä¸€ç»„
        end_index = start_index + group_size
        groups.append(list(range(start_index, end_index)))
        # æ›´æ–°ä¸‹ä¸€ä¸ªç»„çš„èµ·å§‹ç´¢å¼•
        start_index = end_index
    return groups


def image_to_base64(image_path):
    # æ‰“å¼€å›¾åƒæ–‡ä»¶å¹¶è¯»å–å…¶äºŒè¿›åˆ¶å†…å®¹
    with open(image_path, 'rb') as file:
        # è¯»å–æ–‡ä»¶å†…å®¹
        binary_data = file.read()

    # å°†äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸º Base64 å­—ç¬¦ä¸²
    base64_data = base64.b64encode(binary_data)

    # å°†å­—èŠ‚å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    base64_string = base64_data.decode('utf-8')

    return base64_string


def image_to_url(image_path):
    return minio_util.fput_file("inference", image_path)


def _parse_text(text):
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f"<br></code></pre>"
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", r"\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>" + line
    text = "".join(lines)
    return text


def _remove_image_special(text):
    text = text.replace('<ref>', '').replace('</ref>', '')
    return re.sub(r'<box>.*?(</box>|$)', '', text)


def change_prompt_template(_input_text, _save_parameter):
    _save_parameter['prompt_template'] = _input_text
    return _input_text, _save_parameter


def change_temperature(_sld_value, _save_parameter):
    _save_parameter['temperature'] = _sld_value
    return _sld_value, _save_parameter


def change_top_k(_sld_value, _save_parameter):
    _save_parameter['top_k'] = _sld_value
    return _sld_value, _save_parameter


def change_top_p(_sld_value, _save_parameter):
    _save_parameter['top_p'] = _sld_value
    return _sld_value, _save_parameter


def _launch_chat(args):
    uploaded_file_dir = os.environ.get("GRADIO_TEMP_DIR") or "./upload"
    # å®šä¹‰æ”¯æŒæ¨¡å‹åˆ—è¡¨
    qwen_vl_chat_model = QwenVLChatModel(model_name="custom-qwen-vl-chat", url=VLLM_ENDPOINT + "/v1/chat/completions",
                                         max_tokens=1000, temperature=0.8)
    qwen_vl2_instruct_model = QwenVLChatModel(model_name="Qwen2-VL-7B-Instruct",
                                              url=VLLM_ENDPOINT + "/v1/chat/completions",
                                              max_tokens=1000, temperature=0.8)
    internvl_8b_model = InternVLModel(model_name="InternVL2-8B", url=LM_DEPLOY_ENDPOINT + "/v1/chat/completions",
                                      max_tokens=1000, temperature=0.9)
    internvl_8b_model_lora = InternVLModel(model_name="InternVL2-8B-Lora",
                                           url=LM_DEPLOY_ENDPOINT + "/v1/chat/completions",
                                           max_tokens=1000, temperature=0.9)
    internvl_26b_model = InternVLModel(model_name="InternVL2-26B", url=LM_DEPLOY_ENDPOINT + "/v1/chat/completions",
                                       max_tokens=2000, temperature=0.8)
    glm_4v_model = GLM4VModel(model_name="glm-4v-9b", url=LM_DEPLOY_ENDPOINT + "/v1/chat/completions", max_tokens=200,
                              temperature=0.9)
    support_models = {
        "Qwen-Vl-Chat": qwen_vl_chat_model,
        "Qwen-VL2-Instruct": qwen_vl2_instruct_model,
        "InternVL2-8B": internvl_8b_model,
        "InternVL2-8B-Lora": internvl_8b_model_lora,
        "InternVL-26B": internvl_26b_model,
        "glm_4v_model": glm_4v_model,
    }

    def predict(model_infos, parameter_info):
        with_history = False
        for model_info in model_infos:
            chat_query = model_info['query']
            if len(chat_query) == 0:
                continue
            messages = []
            content = []
            request_chat = copy.deepcopy(model_info['query'])
            if with_history:
                request_chat = copy.deepcopy(model_info['history'])
            for q, a in request_chat:
                if q is None:
                    q = ""
                if isinstance(q, (tuple, list)):
                    print("å‘é€æ¶ˆæ¯å›¾ç‰‡ï¼š" + image_to_url(q[0]))
                    content.append({
                        'type': "image_url",
                        'image_url': {
                            'url': image_to_url(q[0])
                        }
                    })
                else:
                    print("User: " + ("æ— " if q is None else q))
                    content.append({
                        'type': "text",
                        'text': parameter_info["prompt_template"].replace("{user_input}", q)
                    })
                    messages.append({'role': 'user', 'content': content})
                    messages.append({'role': 'assistant', 'content': [{'type': 'text', 'text': a}]})
                    content = []
            messages.pop()
            print(f"å‘é€message: {json.dumps(messages, ensure_ascii=False)}")
            response_text = support_models[model_info['modelName']].call(messages)

            print(response_text)
            # é‡æ„ç»“æ„
            model_info['lastQuery'] = model_info['query']
            model_info['query'] = []
            query_text = model_info['history'][-1][0]
            if len(str(query_text).strip()) == 0:
                query_text = None
            model_info['history'][-1] = (query_text, response_text)

            print("Model Response: " + response_text)
            # model_info['history'] = model_info['history'][-10:]
            print(model_infos)
        return model_infos

    def regenerate(model_infos, parameter_info):
        has_last_query = False
        for model_info in model_infos:
            if len(model_info['lastQuery']) > 0:
                has_last_query = True
                model_info['query'] = model_info['lastQuery']
                model_info['lastQuery'] = []
                model_info['history'][-1] = (model_info['history'][-1][0], None)
        if has_last_query:
            return predict(model_infos, parameter_info)
        return model_infos

    def add_text(model_infos, input_text):
        if len(str(input_text).strip()) == 0:
            input_text = None
        for model_info in model_infos:
            model_info['query'] = model_info['query'] + [(input_text, None)]
            model_info['history'] = model_info['history'] + [(input_text, None)]
        return model_infos

    def add_file(model_infos, file, query):
        for model_info in model_infos:
            model_info['query'] = model_info['query'] + [((file.name,), None)]
            model_info['history'] = model_info['history'] + [((file.name,), None)]
        return model_infos, query

    def reset_user_input():
        return gr.update(value="")

    def reset_state(model_infos):
        for model_info in model_infos:
            model_info['query'] = []
            model_info['history'] = []
            model_info['lastQuery'] = []
        return model_infos

    def add_model(model_infos):
        model_infos.append({
            "modelName": "Qwen-Vl-Chat",
            "query": [],
            "history": [],
            "lastQuery": [],
        })
        return model_infos

    render_chatbot_js = """
    <script>
    
    // è§‚å¯Ÿå™¨çš„é…ç½®ï¼ˆéœ€è¦è§‚å¯Ÿä»€ä¹ˆå˜åŠ¨ï¼‰
    const config = { attributes: true, childList: true, subtree: true };
     
    // å½“è§‚å¯Ÿåˆ°å˜åŠ¨æ—¶æ‰§è¡Œçš„å›è°ƒå‡½æ•°
    const callback = function(mutationsList, observer) {
        // Use traditional 'for loops' for IE 11
        for(let mutation of mutationsList) {
            if (mutation.type === 'childList') {
                console.log('A child node has been added or removed.');
                // ä½¿ç”¨å‡½æ•°æŸ¥è¯¢ id ä»¥ 'chatbot_' å¼€å¤´çš„æ‰€æœ‰å…ƒç´ 
                var chatContainers = queryElementsByIdPrefix('chatbot_');
                // æ›´æ”¹æ»šåŠ¨æ¡
                chatContainers.forEach(chatContainer => {
                    var children = chatContainer.querySelector('div[aria-label="chatbot conversation"]');
                    children.scrollTop = children.scrollHeight;
                });
            }
            else if (mutation.type === 'attributes') {
                console.log('The ' + mutation.attributeName + ' attribute was modified.');
            }
        }
    };

    
    function queryElementsByIdPrefix(prefix) {
        // è·å–æ‰€æœ‰å¸¦æœ‰ id å±æ€§çš„å…ƒç´ 
        var allElementsWithId = document.querySelectorAll("[id]");
        // åˆ›å»ºä¸€ä¸ªæ•°ç»„æ¥å­˜å‚¨åŒ¹é…çš„å…ƒç´ 
        var matchingElements = [];
        // éå†æ‰€æœ‰å…ƒç´ å¹¶æ£€æŸ¥ id æ˜¯å¦ä»¥æŒ‡å®šçš„å‰ç¼€å¼€å§‹
        for (var i = 0; i < allElementsWithId.length; i++) {
            var element = allElementsWithId[i];
            if (element.id.startsWith(prefix)) {
                matchingElements.push(element);
            }
        }
        // è¿”å›åŒ¹é…çš„å…ƒç´ åˆ—è¡¨
        return matchingElements;
    }
    
    // å­˜å‚¨å½“å‰chatbotç»„ä»¶
    const chatbotMap = new Map();
    
    // å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†å®šæ—¶ä»»åŠ¡
    function checkAndProcessChatbotElements() {
        // æŸ¥è¯¢æ‰€æœ‰ chatbot å…ƒç´ 
        var chatbotElements = queryElementsByIdPrefix('chatbot_');
        
        // éå†å½“å‰æŸ¥æ‰¾åˆ°çš„ chatbot å…ƒç´ 
        chatbotElements.forEach(function(element) {
            // æ£€æŸ¥è¯¥å…ƒç´ æ˜¯å¦å·²ç»åœ¨ chatbotMap ä¸­
            if (!chatbotMap.has(element.id)) {
                // å¦‚æœæ˜¯æ–°å…ƒç´ ï¼Œåˆ™æ·»åŠ åˆ° chatbotMap
                console.log(`New chatbot element found with ID: ${element.id}`);
                // ä»¥ä¸Šè¿°é…ç½®å¼€å§‹è§‚å¯Ÿç›®æ ‡èŠ‚ç‚¹
                var children = element.querySelector('div[aria-label="chatbot conversation"]');
                // å­˜å‚¨
                chatbotMap.set(element.id, {"element": element, "observer": observer});
            }
        });
    
        // éå† chatbotMap ä¸­çš„æ‰€æœ‰å…ƒç´ 
        chatbotMap.forEach(function(element, id) {
            // æ£€æŸ¥è¯¥å…ƒç´ æ˜¯å¦è¿˜åœ¨å½“å‰æŸ¥æ‰¾åˆ°çš„ chatbot å…ƒç´ ä¸­
            if (!chatbotElements.some(e => e.id === id)) {
                // å¦‚æœä¸åœ¨ï¼Œåˆ™æ‰“å°ä¸€æ¡æ¶ˆæ¯è¡¨ç¤ºå…ƒç´ è¢«åˆ é™¤
                console.log(`Chatbot element with ID ${id} was removed`);
                //åˆ é™¤å…ƒç´ 
                chatbotMap.delete(id);
            }
        });
    }
        
    //if (window.chatbotScroll) {
    //    window.clearInterval(window.chatbotScroll);
    //}
    // ç›‘å¬æ‰€æœ‰chatbotç»„ä»¶
    // window.chatbotScroll = window.setInterval(checkAndProcessChatbotElements, 500);
    
    // ç›‘å¬ç•Œé¢å…ƒç´ æ›´æ–°
    setTimeout(function(){
        const targetNode = document.getElementById('component-5');
        // åˆ›å»ºä¸€ä¸ªè§‚å¯Ÿå™¨å®ä¾‹å¹¶ä¼ å…¥å›è°ƒå‡½æ•°
        const observer = new MutationObserver(callback);
        // ä»¥ä¸Šè¿°é…ç½®å¼€å§‹è§‚å¯Ÿç›®æ ‡èŠ‚ç‚¹
        observer.observe(targetNode, config);
    },2000);
    </script>
    
    """

    with gr.Blocks(head=render_chatbot_js) as chat_block:
        gr.Markdown("""<center><font size=8>æ¨¡å‹æµ‹è¯•å¹³å°</center>""")
        gr.Markdown("""<center><font size=3>æœ¬WebUIåŸºäºGradioæ‰“é€ ï¼Œä»…ä¾›å†…éƒ¨å¤šæ¨¡æ€å¤§æ¨¡å‹æ•ˆæœæµ‹è¯•ã€‚ @Rain</center>""")

        # å®šä¹‰æ¨¡å‹æ•° é»˜è®¤æ˜¯ä¸€ä¸ªæ¨¡å‹
        model_info_state = gr.State([
            {
                "modelName": "Qwen-Vl-Chat",
                "query": [],
                "history": [],
                "lastQuery": [],
            }
        ])
        add_model_btn = gr.Button("æ·»åŠ æ¨¡å‹")  # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œç”¨äºå¢åŠ æ¨¡å‹ç»„ä»¶
        add_model_btn.click(add_model, model_info_state, model_info_state)  # å°†æŒ‰é’®ç‚¹å‡»äº‹ä»¶ç»‘å®šåˆ°lambdaå‡½æ•°ï¼Œæ¯æ¬¡ç‚¹å‡»æ—¶å°†æ¨¡å‹æ•°é‡åŠ 1

        # å®šä¹‰æ¸²æŸ“å¾…åŠäº‹é¡¹çš„å‡½æ•°
        @gr.render(inputs=model_info_state)
        def render_models(model_infos):
            # æ ¹æ®model_numåˆ†ç»„ çœ‹éœ€è¦éå†å¤šå°‘è¡Œï¼Œå®šä¹‰ä¸€è¡Œæœ€å¤š2ä¸ªç»„ä»¶
            row_max_num = 2
            group = split_into_groups(len(model_infos), row_max_num)
            # æ¸²æŸ“æ¨¡å‹ç»„ä»¶
            for item_list in group:
                # åˆ†ç»„çœ‹éœ€è¦å¤šå°‘è¡Œ
                with gr.Row():  # å¯¹äºæ¯ä¸ªæœªå®Œæˆçš„ä»»åŠ¡ï¼Œåˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨
                    for index in item_list:
                        with gr.Column(scale=1):
                            def select_model(select_value, select_index=index):
                                model_infos[select_index]['modelName'] = select_value
                                return select_value

                            with gr.Row():
                                dropdown = gr.Dropdown(value=model_infos[index]['modelName'],
                                                       choices=list(support_models.keys()),
                                                       label="Select Model", allow_custom_value=True, scale=11)
                                dropdown.select(select_model, [dropdown], [dropdown])
                                delete_btn = gr.Button("åˆ é™¤", variant="stop", scale=1, min_width=8)

                                # å®šä¹‰åˆ é™¤ä»»åŠ¡çš„å‡½æ•°
                                def delete(model_index=index):
                                    print(f"åˆ é™¤ç´¢å¼•{model_index}")
                                    model_infos.pop(model_index)
                                    return model_infos

                                delete_btn.click(delete, None, [model_info_state])  # ç»‘å®šæŒ‰é’®ç‚¹å‡»åˆ°deleteå‡½æ•°

                            chatbot = gr.Chatbot(value=model_infos[index]['history'],
                                                 label=model_infos[index]['modelName'], height=500,
                                                 show_copy_button=True, bubble_full_width=False,
                                                 elem_id=f"chatbot_{generate_random_string(4)}")

        parameter = gr.State({
            "prompt_template": DEFAULT_PROMPT_TEMPLATE,
            "temperature": 0.9,
            "top_k": 40,
            "top_p": 1
        })
        query = gr.Textbox(lines=1, label='Input')
        with gr.Row():
            addfile_btn = gr.UploadButton("ğŸ“ Upload (ä¸Šä¼ æ–‡ä»¶)", file_types=["image"])
            submit_btn = gr.Button("ğŸš€ Submit (å‘é€)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")
            empty_bin = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")

        query.submit(add_text, [model_info_state, query], [model_info_state]).then(
            predict, [model_info_state, parameter], [model_info_state], show_progress=True
        )
        query.submit(reset_user_input, [], [query])
        submit_btn.click(add_text, [model_info_state, query], [model_info_state]).then(
            predict, [model_info_state, parameter], [model_info_state], show_progress=True
        )
        submit_btn.click(reset_user_input, [], [query])
        empty_bin.click(reset_state, [model_info_state], [model_info_state], show_progress="full")
        regen_btn.click(regenerate, [model_info_state, parameter], [model_info_state],
                        show_progress="full")
        addfile_btn.upload(add_file, [model_info_state, addfile_btn, query],
                           [model_info_state, query],
                           show_progress="full")

        with gr.Blocks() as demo:
            with gr.Tab("æ—¥å¿—"):
                logs = gr.Textbox(lines=10)
                chat_block.load(read_logs, None, logs, every=1)
            with gr.Tab("é«˜çº§é…ç½®"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # system prompt
                        prompt_template_tbx = gr.Textbox(lines=10, label="Prompt Template",
                                                         value=parameter.value["prompt_template"],
                                                         interactive=True)  # prompt
                        prompt_template_tbx.blur(change_prompt_template, [prompt_template_tbx, parameter],
                                                 [prompt_template_tbx, parameter])
                    with gr.Column(scale=1):
                        # config
                        temperature_sld = gr.Slider(0.0, 1.0, value=parameter.value["temperature"],
                                                    label="temperature", interactive=True)  # temperature
                        top_p_sld = gr.Slider(0.0, 1.0, value=parameter.value["top_p"], label="top_p",
                                              interactive=True)  # top_p
                        top_k_sld = gr.Slider(1, 40, value=parameter.value["top_k"], label="top_k",
                                              interactive=True)  # top_k
                        temperature_sld.input(change_temperature, [temperature_sld, parameter],
                                              [temperature_sld, parameter])
                        top_p_sld.input(change_top_p, [top_p_sld, parameter],
                                        [top_p_sld, parameter])
                        top_k_sld.input(change_top_k, [top_k_sld, parameter],
                                        [top_k_sld, parameter])

        chat_block.queue().launch(
            share=args.share,
            inbrowser=args.inbrowser,
            server_port=args.server_port,
            server_name=args.server_name,
            auth=("admin", 'jtwl998')
        )


def main():
    args = _get_args()
    _launch_chat(args)


if __name__ == '__main__':
    main()
